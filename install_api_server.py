#!/usr/bin/env python3
"""
IndexTTS API æœåŠ¡å™¨ä¾èµ–å®‰è£…å™¨
æ”¯æŒCUDAå¹¶ç¡®ä¿ä¸vLLMå…¼å®¹æ€§
"""

import os
import sys
import subprocess
import platform

def run_command(cmd, description="", check=True):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    print(f"\n{'='*50}")
    print(f"æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {cmd}")
    print('='*50)

    try:
        result = subprocess.run(cmd, shell=True, check=check, capture_output=True, text=True,
                              encoding='utf-8', errors='ignore')
        if result.stdout:
            print(result.stdout)
        return result
    except subprocess.CalledProcessError as e:
        print(f"é”™è¯¯: {e}")
        if e.stderr:
            print(f"é”™è¯¯è¯¦æƒ…: {e.stderr}")
        if check:
            print("å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
            sys.exit(1)
        return e

def check_nvidia_gpu():
    """æ£€æŸ¥æ˜¯å¦æœ‰NVIDIA GPU"""
    print("æ£€æŸ¥NVIDIA GPUæ”¯æŒ...")
    result = run_command("nvidia-smi", check=False)
    if result.returncode == 0:
        print("âœ“ æ£€æµ‹åˆ°NVIDIA GPU")
        return True
    else:
        print("âŒ æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–nvidia-smiä¸å¯ç”¨")
        return False

def ask_cuda_choice():
    """è¯¢é—®ç”¨æˆ·æ˜¯å¦å®‰è£…CUDAç‰ˆæœ¬"""
    print("\n" + "=" * 60)
    print("PyTorch å®‰è£…é€‰æ‹©")
    print("=" * 60)
    print("1. CUDAç‰ˆæœ¬ (æ¨èï¼Œéœ€è¦NVIDIA GPU)")
    print("2. CPUç‰ˆæœ¬ (é€Ÿåº¦è¾ƒæ…¢ï¼Œä½†å…¼å®¹æ€§æ›´å¥½)")

    while True:
        choice = input("\nè¯·é€‰æ‹© (1 æˆ– 2): ").strip()
        if choice in ['1', '2']:
            return choice == '1'
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")

def check_torch():
    """æ£€æŸ¥PyTorchæ˜¯å¦æ­£ç¡®å®‰è£…"""
    try:
        import torch
        print(f"PyTorch version: {torch.__version__}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPU count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        return True
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False

def main():
    print("IndexTTS API æœåŠ¡å™¨ä¾èµ–å®‰è£…å™¨")
    print("=" * 60)

    # æ£€æŸ¥GPUå¹¶è¯¢é—®ç”¨æˆ·é€‰æ‹©
    has_cuda_gpu = check_nvidia_gpu()

    if has_cuda_gpu:
        use_cuda = ask_cuda_choice()
    else:
        print("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUç‰ˆæœ¬")
        use_cuda = False

    # æ£€æŸ¥pipç‰ˆæœ¬
    print("ç¡®ä¿pipæ˜¯æœ€æ–°çš„...")
    run_command("python -m pip install --upgrade pip", "å‡çº§pip")

    # å®‰è£…PyTorch
    if use_cuda:
        print("å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch...")
        run_command("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121",
                   "å®‰è£…PyTorch CUDA 12.1ç‰ˆæœ¬")
    else:
        print("å®‰è£…CPUç‰ˆæœ¬çš„PyTorch...")
        run_command("pip install torch torchvision torchaudio", "å®‰è£…PyTorch CPUç‰ˆæœ¬")

    # å®‰è£…åŸºç¡€ä¾èµ–
    print("å®‰è£…APIæœåŠ¡å™¨ä¾èµ–...")
    run_command("pip install -r requirements.txt", "å®‰è£…åŸºç¡€ä¾èµ–")

    # éªŒè¯å®‰è£…
    print("\n" + "=" * 60)
    print("éªŒè¯å®‰è£…...")
    print("=" * 60)

    if check_torch():
        print("âœ… PyTorch å®‰è£…æˆåŠŸ")
    else:
        print("âŒ PyTorch å®‰è£…å¤±è´¥")
        sys.exit(1)

    # æµ‹è¯•vLLM
    try:
        import vllm
        print(f"âœ… vLLM å®‰è£…æˆåŠŸï¼Œç‰ˆæœ¬: {vllm.__version__}")
    except ImportError as e:
        print(f"âŒ vLLM å®‰è£…å¤±è´¥: {e}")
        sys.exit(1)

    # æµ‹è¯•FastAPI
    try:
        import fastapi
        print(f"âœ… FastAPI å®‰è£…æˆåŠŸï¼Œç‰ˆæœ¬: {fastapi.__version__}")
    except ImportError as e:
        print(f"âŒ FastAPI å®‰è£…å¤±è´¥: {e}")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("ğŸ‰ IndexTTS API æœåŠ¡å™¨ä¾èµ–å®‰è£…å®Œæˆï¼")
    print("=" * 60)
    print("\nå¯åŠ¨APIæœåŠ¡å™¨:")
    print("python api_server_v2.py --is_fp16 --disable_qwen_emo")
    print("\nå¦‚æœä½¿ç”¨CUDAï¼Œå»ºè®®æ·»åŠ  --host 0.0.0.0 ä»¥å…è®¸è¿œç¨‹è®¿é—®:")
    print("python api_server_v2.py --is_fp16 --disable_qwen_emo --host 0.0.0.0")

if __name__ == "__main__":
    main()